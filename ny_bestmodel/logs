/home/bigdyl/jinsung/DeepMove/codes/main.py
# coding: utf-8
from __future__ import print_function
from __future__ import division

import torch
# from tensorboardX import SummaryWriter
import torch.nn as nn
import torch.optim as optim

import sys
import os

import json
import time
import argparse
import numpy as np
from json import encoder
import logging

encoder.FLOAT_REPR = lambda o: format(o, '.3f')

from train import run_simple, RnnParameterData, generate_input_history, markov, \
    generate_input_long_history, generate_input_long_history2
from model import TrajPreSimple, TrajPreAttnAvgLongUser, TrajPreLocalAttnLong, TrajPreAttnAvgLongUser2,TrajPreAttnAvgLongUser2_fine

def run(args):
    parameters = RnnParameterData(loc_emb_size=args.loc_emb_size, uid_emb_size=args.uid_emb_size,
                                  voc_emb_size=args.voc_emb_size, tim_emb_size=args.tim_emb_size,
                                  hidden_size=args.hidden_size, dropout_p=args.dropout_p,
                                  data_name=args.data_name, lr=args.learning_rate,
                                  lr_step=args.lr_step, lr_decay=args.lr_decay, L2=args.L2, rnn_type=args.rnn_type,
                                  optim=args.optim, attn_type=args.attn_type,
                                  clip=args.clip, epoch_max=args.epoch_max, history_mode=args.history_mode,
                                  model_mode=args.model_mode, data_path=args.data_path, save_path=args.save_path, model_method = args.model_method, pretrain = args.pretrain)
    logger.info('*' * 15 + 'loaded parameters' + '*' * 15)
    # parameters.write_tsv()
    # sys.exit()

    argv = {'loc_emb_size': args.loc_emb_size, 'uid_emb_size': args.uid_emb_size, 'voc_emb_size': args.voc_emb_size,
            'tim_emb_size': args.tim_emb_size, 'hidden_size': args.hidden_size,
            'dropout_p': args.dropout_p, 'data_name': args.data_name, 'learning_rate': args.learning_rate,
            'lr_step': args.lr_step, 'lr_decay': args.lr_decay, 'L2': args.L2, 'act_type': 'selu',
            'optim': args.optim, 'attn_type': args.attn_type, 'clip': args.clip, 'rnn_type': args.rnn_type,
            'epoch_max': args.epoch_max, 'history_mode': args.history_mode, 'model_mode': args.model_mode}
    logger.info('model_mode:{} history_mode:{} users:{}'.format(
        parameters.model_mode, parameters.history_mode, parameters.uid_size))
    
    
    if parameters.model_mode in ['simple', 'simple_long']:
        model = TrajPreSimple(parameters=parameters).cuda()
    elif parameters.model_mode == 'attn_avg_long_user':
        model = TrajPreAttnAvgLongUser2(parameters=parameters).cuda()
    elif parameters.model_mode == 'attn_local_long':
        model = TrajPreLocalAttnLong(parameters=parameters).cuda()


    if args.pretrain == 1:
        print("fine")
        if 'taxi' in args.data_name:  # taxi
            model.load_state_dict(torch.load("/localpath/taxi_noattn_2/checkpoint/ep_19.m"))
        elif 'la' in args.data_name:  # la
            model.load_state_dict(torch.load("/localpath/DeepMove/codes/la_test3/checkpoint/ep_3.m"))
        elif 'foursquare' in args.data_name:  # ny
            model.load_state_dict(torch.load("/localpath/jinsung/DeepMove/ny_no_attn/checkpoint/ep_11.m"))

        pretrained_dict = model.state_dict()
        fine_tune = TrajPreAttnAvgLongUser2_fine(parameters=parameters).cuda()
        new_model_dict = fine_tune.state_dict()
        pretrained_dict =  {k: v for k, v in pretrained_dict.items() if k in new_model_dict}
        new_model_dict.update(pretrained_dict)
        fine_tune.load_state_dict(new_model_dict)
        model = fine_tune

        for p, par in enumerate(model.parameters()):
            if list(new_model_dict.keys())[p] in list(pretrained_dict.keys()):
                par.requires_grad=False
        
        criterion = nn.NLLLoss().cuda()
        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=parameters.lr, weight_decay=parameters.L2)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=parameters.lr_step, factor=parameters.lr_decay, threshold=1e-3)
        for p in model.parameters():
            p.requires_grad = True

    elif args.pretrain == 0:
        print('train')
        criterion = nn.NLLLoss().cuda()
        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=parameters.lr, weight_decay=parameters.L2) 
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=parameters.lr_step, factor=parameters.lr_decay, threshold=1e-3)
    else:
        print("test")
        if 'taxi' in args.data_name:  # taxi
            model.load_state_dict(torch.load("/localpath/DeepMove/taxi_noattn_2/checkpoint/ep_19.m"))
        elif 'la' in args.data_name:  # la
            model.load_state_dict(torch.load("/home/bigdyl/jinsung/DeepMove/la_ablation_tim5_loc50/checkpoint/ep_22.m"))
            #model.load_state_dict(torch.load("/home/bigdyl/jinsung/DeepMove/la_ablation_tim5_loc100/checkpoint/ep_13.m"))
        elif 'foursquare' in args.data_name:  # ny
            model.load_state_dict(torch.load("/localpath/DeepMove/ny_ablation_loc1000/checkpoint/ep_20.m"))

        criterion = nn.NLLLoss().cuda()
        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=parameters.lr, weight_decay=parameters.L2) 
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=parameters.lr_step, factor=parameters.lr_decay, threshold=1e-3)

    if 'max' in parameters.model_mode:
        parameters.history_mode = 'max'
    elif 'avg' in parameters.model_mode:
        parameters.history_mode = 'avg'
    else:
        parameters.history_mode = 'whole'

    lr = parameters.lr
    metrics = {'train_loss': [], 'valid_loss': [], 'accuracy': [], 'valid_acc': {}}

    model_parameters = filter(lambda p: p.requires_grad, model.parameters())
    params = sum([np.prod(p.size()) for p in model_parameters])
    logger.info(params)
    print(params)
    candidate = parameters.data_neural.keys()  # list(uid)

    st = time.time()

    if 'long' in parameters.model_mode:
        long_history = True
    else:
        long_history = False

    if long_history is False:
        data_train, train_idx = generate_input_history(parameters.data_neural, 'train', mode2=parameters.history_mode,
                                                       candidate=candidate)
        data_test, test_idx = generate_input_history(parameters.data_neural, 'test', mode2=parameters.history_mode,
                                                     candidate=candidate)
    elif long_history is True:
        if parameters.model_mode == 'simple_long':
            data_train, train_idx = generate_input_long_history2(parameters.data_neural, 'train', candidate=candidate)
            data_test, test_idx = generate_input_long_history2(parameters.data_neural, 'test', candidate=candidate)
            data_valid, valid_idx = generate_input_long_history2(parameters.data_neural, 'valid', candidate=candidate)
        else:
            data_train, train_idx = generate_input_long_history(parameters.data_neural, 'train', candidate=candidate)
            data_test, test_idx = generate_input_long_history(parameters.data_neural, 'test', candidate=candidate)
            data_valid, valid_idx = generate_input_long_history(parameters.data_neural, 'valid', candidate=candidate)


    logger.info('*' * 15 + 'generated input: {}'.format(time.time() - st) + '*' * 15)
    SAVE_PATH = args.save_path
    tmp_path = 'checkpoint/'
    if not os.path.exists(SAVE_PATH + tmp_path):  # create checkpoint
        os.makedirs(SAVE_PATH + tmp_path)

    logger.info('*' * 15 + 'start training' + '*' * 15)
    for epoch in range(parameters.epoch):
        if args.pretrain == 2:
            break
        st = time.time()
        if args.pretrain == 0 or args.pretrain == 1:
            model, avg_loss = run_simple(data_train, train_idx, 'train', lr, parameters.clip, model, optimizer,
                                        criterion, parameters.model_mode)
            logger.info('==>Train Epoch:{:0>2d} Loss:{:.4f} lr:{} time:{}'.format(epoch, avg_loss, lr, time.time()-st))
            metrics['train_loss'].append(avg_loss)
            
        # validation
        avg_loss, avg_acc, users_acc = run_simple(data_valid, valid_idx, 'test', lr, parameters.clip, model,
                                                optimizer, criterion, parameters.model_mode,
                                                #   grid_eval=args.grid_eval,  # accuracy eval시에만 grid mapping
                                                grid=parameters.grid_lookup)
        logger.info('==>Validation Acc:{:.4f} Loss:{:.4f}'.format(avg_acc, avg_loss))

        metrics['valid_loss'].append(avg_loss)
        metrics['accuracy'].append(avg_acc)
        metrics['valid_acc'][epoch] = users_acc

        save_name_tmp = 'ep_' + str(epoch) + '.m'
        torch.save(model.state_dict(), SAVE_PATH + tmp_path + save_name_tmp)

        scheduler.step(avg_acc)
        lr_last = lr
        lr = optimizer.param_groups[0]['lr']
        if lr_last > lr:
            logger.info('lr_last > lr ... visit main.py line 152')
            load_epoch = np.argmax(metrics['accuracy'])
            load_name_tmp = 'ep_' + str(load_epoch) + '.m'
            model.load_state_dict(torch.load(SAVE_PATH + tmp_path + load_name_tmp))
            logger.info('load epoch={} model state'.format(load_epoch))
        if epoch == 0:
            logger.info('single epoch time cost:{}'.format(time.time() - st))
        if lr <= 0.9 * 1e-5:
            break
        if args.pretrain == 2:
            break

    logger.info('*' * 15 + 'start testing' + '*' * 15)

    avg_loss, avg_acc, users_acc = run_simple(data_test, test_idx, 'test', lr, parameters.clip, model,
                                              optimizer, criterion, parameters.model_mode)

    logger.info('==>Test Top1 Acc:{:.4f} Top5 Acc:{:.4f} Top10 Acc:{:.4f} MRR:{:.4f} Loss:{:.4f}'.format(
                avg_acc, 
                np.mean([users_acc[x][1] for x in users_acc]),
                np.mean([users_acc[x][2] for x in users_acc]),
                np.mean([users_acc[x][3] for x in users_acc]),  # overall mrr
                avg_loss))

    metrics['valid_loss'].append(avg_loss)
    metrics['accuracy'].append(avg_acc)
    metrics['valid_acc'][epoch] = users_acc

    save_name = 'res'
    json.dump({'args': argv, 'metrics': metrics}, fp=open(SAVE_PATH + save_name + '.rs', 'w'), indent=4)
    metrics_view = {'train_loss': [], 'valid_loss': [], 'accuracy': []}
    for key in metrics_view:
        metrics_view[key] = metrics[key]
    #json.dump({'args': argv, 'metrics': metrics_view}, fp=open(SAVE_PATH + save_name + '.txt', 'w'), indent=4)
    #torch.save(model.state_dict(), SAVE_PATH + save_name + '.m')

    return avg_acc


def load_pretrained_model(config):
    if 'taxi' in config.data_name:  # taxi
        res = json.load(open("./taxi/ep6test/res.txt"))
    elif 'la' in config.data_name:  # la
        res = json.load(open("./la/final_model/res.txt"))
    elif 'foursquare' in config.data_name:  # ny
        #res = json.load(open("./ny/final_model/res.txt"))
        res = json.load(open("/home/bigdyl/jinsung/DeepMove/codes/ny_test_2/res.rs"))
    args = Settings(config, res["args"])
    return args


def get_logger(logpath, filepath, package_files=[], displaying=True, saving=True, debug=False):
    logger = logging.getLogger()
    if debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logger.setLevel(level)
    if saving:
        info_file_handler = logging.FileHandler(logpath, mode="a")
        info_file_handler.setLevel(level)
        logger.addHandler(info_file_handler)
    if displaying:
        console_handler = logging.StreamHandler()
        console_handler.setLevel(level)
        logger.addHandler(console_handler)
    logger.info(filepath)
    with open(filepath, "r") as f:
        logger.info(f.read())

    for f in package_files:
        logger.info(f)
        with open(f, "r") as package_f:
            logger.info(package_f.read())

    return logger


class Settings(object):
    def __init__(self, config, res):
        self.data_path = config.data_path
        self.save_path = config.save_path
        self.data_name = res["data_name"]
        self.epoch_max = res["epoch_max"]
        self.learning_rate = res["learning_rate"]
        self.lr_step = res["lr_step"]
        self.lr_decay = res["lr_decay"]
        self.clip = res["clip"]
        self.dropout_p = res["dropout_p"]
        self.rnn_type = res["rnn_type"]
        self.attn_type = res["attn_type"]
        self.L2 = res["L2"]
        self.history_mode = res["history_mode"]
        self.model_mode = res["model_mode"]
        self.optim = res["optim"]
        self.hidden_size = res["hidden_size"]
        self.tim_emb_size = res["tim_emb_size"]
        self.loc_emb_size = res["loc_emb_size"]
        self.uid_emb_size = res["uid_emb_size"]
        self.voc_emb_size = res["voc_emb_size"]
        self.model_method = config.model_method
        self.pretrain = 1


if __name__ == '__main__':
    np.random.seed(1)
    torch.manual_seed(1)

    parser = argparse.ArgumentParser()
    parser.add_argument('--loc_emb_size', type=int, default=500, help="location embeddings size")
    parser.add_argument('--uid_emb_size', type=int, default=40, help="user id embeddings size")
    parser.add_argument('--voc_emb_size', type=int, default=50, help="words embeddings size")
    parser.add_argument('--tim_emb_size', type=int, default=10, help="time embeddings size")
    parser.add_argument('--hidden_size', type=int, default=500)
    parser.add_argument('--dropout_p', type=float, default=0.3)
    parser.add_argument('--data_name', type=str, default='foursquare2')
    parser.add_argument('--learning_rate', type=float, default=5 * 1e-5)
    parser.add_argument('--lr_step', type=int, default=2)  # 3
    parser.add_argument('--lr_decay', type=float, default=0.1)
    parser.add_argument('--optim', type=str, default='Adam', choices=['Adam', 'SGD'])
    parser.add_argument('--L2', type=float, default=1 * 1e-5, help=" weight decay (L2 penalty)")
    parser.add_argument('--clip', type=float, default=5.0)
    parser.add_argument('--epoch_max', type=int, default=30)
    parser.add_argument('--history_mode', type=str, default='avg', choices=['max', 'avg', 'whole'])
    parser.add_argument('--rnn_type', type=str, default='LSTM', choices=['LSTM', 'GRU', 'RNN'])
    parser.add_argument('--attn_type', type=str, default='dot', choices=['general', 'concat', 'dot'])
    parser.add_argument('--data_path', type=str, default='../data/Foursquare')
    parser.add_argument('--save_path', type=str, default='../out/')
    parser.add_argument('--model_mode', type=str, default='attn_avg_long_user',
                        choices=['simple', 'simple_long', 'attn_avg_long_user', 'attn_local_long'])
    parser.add_argument('--load_checkpoint', type=int, default=None)  # checkpoint to load

    parser.add_argument('--model_method', type=int, default=0) # model_method = 0(G0E) 1(L2E) 2(G2E) 3(G5E)
    
    parser.add_argument('--jump', type=str, default='yes')
    parser.add_argument('--jump_method', type=str, default='gru')
    parser.add_argument('--jump_time', type=int, default=5)
    parser.add_argument('--pretrain', type=int, default=1)

    args = parser.parse_args()

    if not os.path.exists(args.save_path):
        os.makedirs(args.save_path)
    logger = get_logger(logpath=os.path.join(args.save_path, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    ours_acc = run(args)

Namespace(L2=1e-05, attn_type='dot', clip=5.0, data_name='foursquare2', data_path='/home/bigdyl/jinsung/DeepMove/data/', dropout_p=0.3, epoch_max=50, hidden_size=100, history_mode='avg', jump='yes', jump_method='gru', jump_time=5, learning_rate=0.005, load_checkpoint=None, loc_emb_size=100, lr_decay=0.1, lr_step=2, model_method=1, model_mode='attn_avg_long_user', optim='Adam', pretrain=0, rnn_type='LSTM', save_path='/home/bigdyl/jinsung/DeepMove/ny_{1}_{60}_{10}_{100}_{0.3}_{0.005}/', tim_emb_size=10, uid_emb_size=60, voc_emb_size=50)
***************loaded parameters***************
model_mode:attn_avg_long_user history_mode:avg users:886
2956908
***************generated input: 0.6084456443786621***************
***************start training***************
==>Train Epoch:00 Loss:7.8702 lr:0.005 time:243.17921829223633
==>Validation Acc:0.0780 Loss:7.2433
single epoch time cost:264.8084046840668
==>Train Epoch:01 Loss:6.4156 lr:0.005 time:241.7553744316101
==>Validation Acc:0.1150 Loss:6.7160
==>Train Epoch:02 Loss:5.8612 lr:0.005 time:241.97680139541626
==>Validation Acc:0.1255 Loss:6.5358
==>Train Epoch:03 Loss:5.6111 lr:0.005 time:241.23851776123047
==>Validation Acc:0.1320 Loss:6.4686
==>Train Epoch:04 Loss:5.4929 lr:0.005 time:241.9424545764923
==>Validation Acc:0.1333 Loss:6.4323
==>Train Epoch:05 Loss:5.4384 lr:0.005 time:241.00401854515076
==>Validation Acc:0.1355 Loss:6.4223
==>Train Epoch:06 Loss:5.3922 lr:0.005 time:243.66868901252747
==>Validation Acc:0.1324 Loss:6.4370
==>Train Epoch:07 Loss:5.3924 lr:0.005 time:238.84175872802734
==>Validation Acc:0.1306 Loss:6.4122
==>Train Epoch:08 Loss:5.3677 lr:0.005 time:242.53448367118835
==>Validation Acc:0.1357 Loss:6.4385
==>Train Epoch:09 Loss:5.3562 lr:0.005 time:242.81379771232605
==>Validation Acc:0.1335 Loss:6.4065
==>Train Epoch:10 Loss:5.3466 lr:0.005 time:247.22591423988342
==>Validation Acc:0.1321 Loss:6.4367
==>Train Epoch:11 Loss:5.3449 lr:0.005 time:239.4803729057312
==>Validation Acc:0.1321 Loss:6.4077
lr_last > lr ... visit main.py line 152
load epoch=8 model state
==>Train Epoch:12 Loss:4.6650 lr:0.0005 time:244.28119325637817
==>Validation Acc:0.1477 Loss:6.2965
==>Train Epoch:13 Loss:4.5286 lr:0.0005 time:246.45360040664673
==>Validation Acc:0.1518 Loss:6.2993
==>Train Epoch:14 Loss:4.4836 lr:0.0005 time:246.51851105690002
==>Validation Acc:0.1571 Loss:6.3419
==>Train Epoch:15 Loss:4.4491 lr:0.0005 time:241.0216372013092
==>Validation Acc:0.1573 Loss:6.3266
==>Train Epoch:16 Loss:4.4074 lr:0.0005 time:244.36178731918335
==>Validation Acc:0.1593 Loss:6.3142
==>Train Epoch:17 Loss:4.3571 lr:0.0005 time:246.23349738121033
==>Validation Acc:0.1581 Loss:6.2786
==>Train Epoch:18 Loss:4.3076 lr:0.0005 time:241.584979057312
==>Validation Acc:0.1640 Loss:6.2395
==>Train Epoch:19 Loss:4.2568 lr:0.0005 time:243.73985075950623
==>Validation Acc:0.1689 Loss:6.2311
==>Train Epoch:20 Loss:4.2072 lr:0.0005 time:242.60908770561218
==>Validation Acc:0.1694 Loss:6.2049
==>Train Epoch:21 Loss:4.1711 lr:0.0005 time:242.40952491760254
==>Validation Acc:0.1675 Loss:6.2122
==>Train Epoch:22 Loss:4.1501 lr:0.0005 time:246.33326530456543
==>Validation Acc:0.1611 Loss:6.2202
==>Train Epoch:23 Loss:4.1209 lr:0.0005 time:243.03236722946167
==>Validation Acc:0.1623 Loss:6.2305
lr_last > lr ... visit main.py line 152
load epoch=20 model state
==>Train Epoch:24 Loss:3.9971 lr:5e-05 time:246.3201675415039
==>Validation Acc:0.1701 Loss:6.1942
==>Train Epoch:25 Loss:3.9767 lr:5e-05 time:244.36181163787842
==>Validation Acc:0.1694 Loss:6.1945
==>Train Epoch:26 Loss:3.9464 lr:5e-05 time:246.0470130443573
==>Validation Acc:0.1705 Loss:6.1921
==>Train Epoch:27 Loss:3.9411 lr:5e-05 time:242.632821559906
==>Validation Acc:0.1694 Loss:6.1930
==>Train Epoch:28 Loss:3.9357 lr:5e-05 time:248.39799880981445
==>Validation Acc:0.1704 Loss:6.1898
==>Train Epoch:29 Loss:3.9175 lr:5e-05 time:244.4541049003601
==>Validation Acc:0.1711 Loss:6.1889
==>Train Epoch:30 Loss:3.9199 lr:5e-05 time:241.74053812026978
==>Validation Acc:0.1705 Loss:6.1922
==>Train Epoch:31 Loss:3.9210 lr:5e-05 time:246.5629906654358
==>Validation Acc:0.1714 Loss:6.1900
==>Train Epoch:32 Loss:3.9185 lr:5e-05 time:246.05525612831116
==>Validation Acc:0.1724 Loss:6.1913
==>Train Epoch:33 Loss:3.9135 lr:5e-05 time:245.82635307312012
==>Validation Acc:0.1721 Loss:6.1934
==>Train Epoch:34 Loss:3.9115 lr:5e-05 time:244.31141901016235
==>Validation Acc:0.1714 Loss:6.1916
==>Train Epoch:35 Loss:3.9058 lr:5e-05 time:243.86438822746277
==>Validation Acc:0.1713 Loss:6.1963
lr_last > lr ... visit main.py line 152
load epoch=32 model state
***************start testing***************
==>Test Top1 Acc:0.1537 Top5 Acc:0.3190 Top10 Acc:0.3658 MRR:0.2285 Loss:6.5042
